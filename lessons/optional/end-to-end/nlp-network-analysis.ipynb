{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis of Characters in a Document\n",
    "\n",
    "In this notebook, we look at pairs of characters being mentioned in the plot for the Harry Potter novels (as \n",
    "described in Wikipedia), and do some network analysis on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "import itertools\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to get the content from wikipedia. Normally it's not a good idea to scrape wikipedia like this\n",
    "(there are better ways), but what we are doing here is a general technique that will work on most web pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://en.wikipedia.org/wiki/Harry_Potter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that there's a section in there somewhere for the plot... let's try to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_starts = soup.find('h2', text=\"Plot\")\n",
    "plot_ends = plot_starts.find_next_sibling('h2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find all the content between those two tags and turn it into plaintext sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot = []\n",
    "sib = plot_starts.next_element\n",
    "footnote = re.compile('\\[\\d+\\]')\n",
    "while sib != plot_ends:\n",
    "    sib = sib.next_element\n",
    "    try:\n",
    "        t = sib.text\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    t = footnote.sub(' ', t)\n",
    "    t = t.strip()\n",
    "    if t == '':\n",
    "        continue\n",
    "    if len(t.split(' ')) < 3:\n",
    "        continue\n",
    "    plot.append(t)\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's analyse each of those text chunks using the Spacy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parsed_list = list(map(nlp, plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate through every text chunk, breaking it into sentences, and then within those sentences, grab all the\n",
    "people that are mentioned.\n",
    "\n",
    "An improvement would be to look at the following sentences and look for \"he\" or \"she\" or \"him\" or \"her\" and match\n",
    "them up to the person mentioned in the previous sentence; this is known as _coreference resolution_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "entity_comentions = []\n",
    "for p in parsed_list:\n",
    "    for s in p.sents:\n",
    "        sentences.append(s)\n",
    "        entities = [x.root.text for x in s.ents if x.label_ == 'PERSON']\n",
    "        entity_comentions.append(entities)\n",
    "entity_comentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the `itertools.combinations` function can take a 3-element list and give you pairs of elements that\n",
    "appear in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(itertools.combinations(['Ron', 'Weasley', 'Riddle'], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few words are getting mis-identified as a person. A Pensieve is a thing; Surrey is a place. We don't want\n",
    "Rowling included in our graph, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kill_list = ['Pensieve', 'Surrey', 'awakens', 'Muggles', 'Rowling']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably the most complex cell in this notebook. We look at the people mentioned in each sentence. Then\n",
    "we use `itertools.combinations` to get all the pairs of people mentioned. If they aren't in alphabetical order,\n",
    "put them in order, and if anyone is on the kill list, ignore them. Finally, we'll store the pair in a dictionary and count the number of times that pair is mentioned together.\n",
    "\n",
    "(Improvements might include mapping \"Harry\" to \"Potter\", and \"Weasley\" to either \"Ron\" or \"Ginny\" as appropriate.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = {}\n",
    "for comention in entity_comentions:\n",
    "    if len(comention) < 2:\n",
    "        continue\n",
    "    for pair in itertools.combinations(comention, 2):\n",
    "        # Get them in alphabetical order\n",
    "        if pair[0] > pair[1]:\n",
    "            pair = (pair[1], pair[0])\n",
    "        if pair[0] == pair[1]:\n",
    "            continue\n",
    "        if pair[0] in kill_list:\n",
    "            continue\n",
    "        if pair[1] in kill_list:\n",
    "            continue\n",
    "        pairs[pair] = pairs.get(pair, 0) + 1\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHAT IS A GRAPH?\n",
    "\n",
    "A graph consists of a nodes (or vertices) and are connected by edges. Many types of real-world problems involve dependencies between observations.\n",
    "\n",
    "- Town planners are looking at vehicular flows through a city\n",
    "\n",
    "- Sociologist want to understand how people influence others that they know (if at all)\n",
    "\n",
    "- Biologists want to know how proteins regulate the actions of other proteins\n",
    "\n",
    "- Credit card fraud: vendors and card users are nodes in the network, purchases are edges\n",
    "\n",
    "- Social media networks want to identify groups of close friends\n",
    "\n",
    "(DiGraphs have arrows, MultiGraphs have parallel lines between nodes)\n",
    "\n",
    "We will create a graph showing the connections between characters in Harry Potter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `networkx` library is a convenient library for graph operations. It works adequately for graphs with a few\n",
    "thousands nodes for most algorithms.\n",
    "\n",
    "But be aware it also implements algorithms for some complex problems that scale so poorly that all the world's supercomputers together can't solve them for a thousand node graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.add_edge` method also creates nodes (it calls the `.add_node` method) while it creates the edges. You can\n",
    "optionally specify a weight.\n",
    "\n",
    "Here, the weight of the edge is how often the two characters are mentioned together in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (pair, weight) in pairs.items():\n",
    "    graph.add_edge(pair[0], pair[1], weight=weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small graphs, it's easy to visualise them with the built-in functions in the networkx library. For larger graphs,\n",
    "try exporting to graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree of a node is the number of edges that have one end in the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.degree(graph, 'Harry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.degree(graph, 'Hagrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterising Graphs\n",
    "\n",
    "When we have a graph, we'd like to be able to label the graph if it has certain common characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is it nearly a lattice?\n",
    "\n",
    "- A lattice has every possible edge\n",
    "\n",
    "- It has the maximum possible _density_\n",
    "\n",
    "Density – connectedness of the graph: ratio of edges to total possible number of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there any islands?\n",
    "\n",
    "An _island_ is when there is at least one pair of nodes that have no paths between them.\n",
    "\n",
    "If there are no islands, then the `connected_components` function will return a one-element list. That one element\n",
    "will be the whole graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(nx.connected_components(graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see a document where there are islands of character co-mentions, it means that there are two completely different\n",
    "plots that don't overlap. Or, you have accidentally included a Tolkien book in your analysis of Harry Potter. (An\n",
    "easy mistake to make.)\n",
    "\n",
    "Almost all network analysis assumes full connectivity, which might mean creating a subgraph to analyse:\n",
    "`nx.subgraph(graph, ['Harry'])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World-size\n",
    "\n",
    "#### Average clustering coefficient.\n",
    "\n",
    "If three nodes are connected by at least two edges, what is the probability that they are connected by three?\n",
    "\n",
    "If character A and B get mentioned together, and B and C get mentioned together, what's the probability that A and C\n",
    "will be mentioned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_clustering(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average shortest path\n",
    "\n",
    "Calculate the shortest path between each pair of nodes, divide by the number of pairs. (Computationally expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_shortest_path_length(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                                                                 | Average clustering coefficient is small | Average clustering coefficient is large |\n",
    "|-----------------------------------------------------------------|-----------------------------------------|-----------------------------------------|\n",
    "| Average shortest path is small (less than log(number of nodes)) | Random graph                            | Small world                             |\n",
    "| Average shortest path is large                                  | Chain                                   | Chain of cliques                                 |\n",
    "\n",
    "**Random graphs** are uninteresting: nodes are randomly connected to each other. There is probably no point in analysing\n",
    "this network any further.\n",
    "\n",
    "**Chains** are also uninteresting, although it can be interesting to look at what process is attaching nodes to either end:\n",
    "is one end favoured? Why?\n",
    "\n",
    "**Small world networks** are very interesting and very well studied. You will see these appear very regularly.\n",
    "\n",
    "**Chains of cliques** are very, very rare and we have very few tools that can study them well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.log(len(graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small world networks\n",
    "\n",
    "Have hubs (high degree nodes), and many low-degree nodes. They appear to be more robust against failure of an individual node.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Website navigation menus\n",
    "\n",
    "- Power grids\n",
    "\n",
    "- Telephone call graphs\n",
    "\n",
    "- Social networks\n",
    "\n",
    "- Six degrees of separation (among living people)\n",
    "\n",
    "Whenever you see a small world network, it means that there is an **effect joining clusters which is not just spatial or temporal proximity**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harry Potter is a small world network\n",
    "\n",
    "J.K. Rowling didn't just write random characters together -- she had a plot in mind. So it's not surprising\n",
    "that we see a small-world network here.\n",
    "\n",
    "If we pick a random character, we are likely to pick a character of little importance (e.g. Hagrid could be\n",
    "dropped from the novels without making much difference to the plot.) But if we take a high-degree node out\n",
    "(such as Harry Potter), the novels would be very, very different!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ultra-small worlds are called \"Scale Free\"\n",
    "\n",
    "In an ultra-small world, the distribution of the \"degree” of a node follows a _power law_. E.g. double the number of connections = half as many nodes have that number.\n",
    "\n",
    "You can see if this is happening: do a log-log plot and see if it looks straight\n",
    "\n",
    "Scale-free networks are generally **formed by preferential attachment** -- a new node is added to the network with a probability related to the degree of existing nodes. This happens in internet links for example -- when you add a new link, you\n",
    "probably want to link to an important core network; it's rare to make a link to a minor router."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = pd.Series([nx.degree(graph, node) for node in graph])\n",
    "degree_counts = degrees.value_counts().reset_index()\n",
    "degree_counts.columns = ['degree', 'freq']\n",
    "degree_counts.plot.scatter(x='degree', y='freq', logx=True, logy=True)\n",
    "plt.xlim(0.5, 100)\n",
    "plt.ylim(0.5, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harry Potter characters don't quite appear to be scale-free. But we don't have much data, so it's hard to say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterising Nodes\n",
    "\n",
    "#### What are the most important nodes?\n",
    "\n",
    "We need to calculate some measures of importance (or centrality).\n",
    "\n",
    "There are many techniques, and they don’t generally agree with each other very much. Most only work for the identifying a few important nodes, and can’t distinguish lesser-importance nodes.\n",
    "\n",
    "It's common to add these importance measures as features into a dataframe for supervised learning problems, as they \n",
    "express some concept of how well embedded into a product community a customer is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `networkx` algorithms all return a dictionary with nodes as keys, and the importance measure as the value. To make it\n",
    "a little simpler to work with them, we'll create this convenience function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx_metric(data):\n",
    "    return pd.Series(index=list(data.keys()), data=list(data.values())).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Centrality\n",
    "\n",
    "The number of edges a node has (divided by the number of other nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.degree_centrality(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_metric(nx.degree_centrality(graph)).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Degree centrality says that the Harry Potter novels are about Harry, Voldemort, Ron, Ginny and Pettigrew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closeness Centrality\n",
    "\n",
    "The reciprocal of the sum of the shortest path distances from one node to all n-1 other nodes. Since the sum of distances depends on the number of nodes in the graph, closeness is normalized by the sum of minimum possible distances n-1. Higher values of closeness indicate higher centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_metric(nx.closeness_centrality(graph)).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betweenness Centrality\n",
    "\n",
    "The sum of the fraction of all-pairs shortest paths that pass through the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_metric(nx.betweenness_centrality(graph)).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvector centrality\n",
    "\n",
    "Computes the centrality for a node based on the centrality of its neighbours. A way of imagining this is: if we let\n",
    "some ants loose on the network and they just randomly moved from one node to the next in proportion to the degree of\n",
    "the nodes and the weights of the edges, where would the ants end up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_metric(nx.eigenvector_centrality(graph)).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page Rank\n",
    "\n",
    "Count the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_metric(nx.pagerank(graph)).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOW CAN WE FIND COMMUNITIES?\n",
    "\n",
    "The criteria for finding good communities is similar to that for finding good clusters. \n",
    "\n",
    "We want to maximize intra-community edges while minimizing inter-community edges. \n",
    "\n",
    "Formally, the algorithm tries to maximize the modularity of network, or the fraction of edges that fall within the community minus the expected fraction of edges if the edges were distributed by random. Good communities should have a high number of intra-community edges, so by maximizing the modularity, we detect dense communities that have a high fraction of intra-community edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in nx.clique.find_cliques(graph):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.clique.cliques_containing_node(graph,'Hagrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does find the trio correctly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.clique.cliques_containing_node(graph,'Granger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity\n",
    "\n",
    "Or, we could ask for groups of nodes with a high Jaccard coefficient. The Jaccard coefficient for two nodes is\n",
    "the ratio of (neighbours in common) to (neighbours of either)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nx.jaccard_coefficient(graph, \n",
    "                               [('Harry', \"Ron\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nx.jaccard_coefficient(graph, \n",
    "                               [('Voldemort', \"Hagrid\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Harry and Ron and more similar to each other than Voldemort and Hagrid are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.write_edgelist(graph, 'edgelist.txt')\n",
    "#graph = nx.read_edgelist('edgelist.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.to_pandas_edgelist(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nx.to_pandas_adjacency(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#networkx.from_pandas_edgelist()\n",
    "#networkx.from_pandas_adjacency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lots more algorithms to explore\n",
    "\n",
    "Start here: http://networkx.readthedocs.io/en/stable/reference/algorithms.html and have some fun!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
